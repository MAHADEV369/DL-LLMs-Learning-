{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e41548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA multiheadattention\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.h = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "        \n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        Q = self.Wq(x).reshape(B, N, self.h, self.dk).transpose(1, 2)\n",
    "        K = self.Wk(x).reshape(B, N, self.h, self.dk).transpose(1, 2)\n",
    "        V = self.Wv(x).reshape(B, N, self.h, self.dk).transpose(1, 2)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.dk ** 0.5)\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = A @ V  # (B, h, N, dk)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)\n",
    "        \n",
    "        return self.Wo(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MQA multiqueryattention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.h = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)   # many Q\n",
    "        self.Wk = nn.Linear(d_model, self.dk)   # one K\n",
    "        self.Wv = nn.Linear(d_model, self.dk)   # one V\n",
    "\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        Q = self.Wq(x).reshape(B, N, self.h, self.dk).transpose(1, 2)\n",
    "        \n",
    "        K = self.Wk(x).unsqueeze(1)  # (B, 1, N, dk)\n",
    "        V = self.Wv(x).unsqueeze(1)  # (B, 1, N, dk)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.dk ** 0.5)\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = A @ V  # (B, h, N, dk)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)\n",
    "\n",
    "        return self.Wo(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3eaf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GQA groupedqueryattention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MQA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.h = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)   # many Q\n",
    "        self.Wk = nn.Linear(d_model, self.dk)   # one K\n",
    "        self.Wv = nn.Linear(d_model, self.dk)   # one V\n",
    "\n",
    "        self.Wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        \n",
    "        Q = self.Wq(x).reshape(B, N, self.h, self.dk).transpose(1, 2)\n",
    "        \n",
    "        K = self.Wk(x).unsqueeze(1)  # (B, 1, N, dk)\n",
    "        V = self.Wv(x).unsqueeze(1)  # (B, 1, N, dk)\n",
    "\n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.dk ** 0.5)\n",
    "        A = F.softmax(scores, dim=-1)\n",
    "\n",
    "        out = A @ V  # (B, h, N, dk)\n",
    "        out = out.transpose(1, 2).reshape(B, N, D)\n",
    "\n",
    "        return self.Wo(out)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
